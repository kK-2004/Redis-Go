# Redis_Go 服务 vs Redis 基准对比报告

> 目标：对比 Go 实现的 Redis 服务（端口 6666）与官方 Redis（端口 6379）在相同 `redis-benchmark` 口径下的吞吐与延迟表现，并记录高并发与随机 Key 负载下的差异特征。

---

## 1. 实验对象与基本信息

- **Redis_Go服务（Go）**：监听 `172.31.0.1:6666`
  - 开启AOF
- **官方 Redis（C）**：监听 `172.31.0.1:6379`
  - 关闭AOF

> 说明：本报告仅基于 benchmark 输出做归纳与分析；未引入额外假设（如 CPU 核数、是否容器/WSL、是否有业务流量干扰等）。

**测试环境：**

  操作系统
  - OS: Windows 11 家庭中文版
  - 版本: 10.0.26200 (Windows 11 Build 26200)
  - 架构: 64位

  处理器 (CPU)
  - 型号: 13th Gen Intel(R) Core(TM) i7-13700H
  - 物理核心: 14核
  - 逻辑处理器: 20线程

  内存 (RAM)
  - 总容量: 16 GB

---

## 2. 测试方法与口径

### 2.1 工具与常用参数
- 工具：`redis-benchmark`
- 连接：TCP，`keep alive: 1`
- 客户端多线程：`multi-thread: no`（benchmark 客户端不启用多线程）

### 2.2 已执行测试集
1) **基础命令对比（c=25, n=100000, payload=3B）**
- `SET / GET / HSET`

2) **极高并发连接压力（c=10000, n=100000, payload=3B）**
- `SET`

3) **随机 Key + 更大 payload（c=50, n=1000000, -r 10000）**
- 由于使用 `set key:__rand_int__ value:__rand_int__`，实际 payload 显示为 **61 bytes**
- `SET`

---

## 3. 结果汇总

### 3.1 基础命令对比（n=100000, c=25, 3B）

| 命令 | 服务 | 吞吐 (req/s) | avg (ms) | p50 (ms) | p95 (ms) | p99 (ms) | max (ms) |
|---|---|---:|---:|---:|---:|---:|---:|
| SET | Go(6666) | 80,064.05 | 0.264 | 0.255 | 0.391 | 0.543 | 7.063 |
| SET | Redis(6379) | 35,842.29 | 0.680 | 0.503 | 1.663 | 2.015 | 14.599 |
| GET | Go(6666) | 87,260.03 | 0.244 | 0.239 | 0.359 | 0.479 | 4.743 |
| GET | Redis(6379) | 33,760.97 | 0.724 | 0.567 | 1.679 | 1.991 | 6.871 |
| HSET | Go(6666) | 81,699.35 | 0.261 | 0.247 | 0.391 | 0.607 | 6.479 |
| HSET | Redis(6379) | 33,898.30 | 0.720 | 0.543 | 1.703 | 1.999 | 10.551 |

**结论（c=25）**
- Go(6666) 吞吐约为 Redis(6379) 的 **2.2×–2.6×**
- Go(6666) 的 p95/p99 明显更低，尾延迟优势更突出（常见为 **3×–5×** 级别差距）

---

### 3.2 极高并发连接压力（n=100000, c=10000, 3B, SET）

| 指标 | Go(6666) | Redis(6379) |
|---|---:|---:|
| 吞吐 (req/s) | 93,720.71 | 26,802.47 |
| avg (ms) | 53.355 | 347.582 |
| p50 (ms) | 53.055 | 364.287 |
| p95 (ms) | 71.039 | 387.071 |
| p99 (ms) | 73.855 | 391.167 |
| max (ms) | 77.567 | 397.823 |

**结论（c=10000）**
- Go(6666) 吞吐约为 Redis(6379) 的 **~3.5×**
- 两者延迟均出现数量级抬升（到几十/几百 ms），该现象主要由 **排队等待（queueing）** 主导：当并发连接/请求远超服务速率时，延迟大多来自“等着被处理”，而非单次处理耗时。
- Redis(6379) 在 10k 连接下吞吐下降、延迟显著扩大，表现出更强的连接规模压力敏感性。

---

### 3.3 随机 Key（keyspace=10000）+ 61B payload（n=1000000, c=50, SET）

| 指标 | Go(6666) | Redis(6379) |
|---|---:|---:|
| 吞吐 (req/s) | 94,206.31 | 36,710.72 |
| avg (ms) | 0.441 | 1.429 |
| p50 (ms) | 0.407 | 1.143 |
| p95 (ms) | 0.735 | 3.215 |
| p99 (ms) | 0.919 | 3.927 |
| max (ms) | 12.167 | 30.831 |

**结论（随机 Key + 61B）**
- Go(6666) 吞吐约为 Redis(6379) 的 **~2.57×**
- p95/p99 尾延迟优势更明显：Redis 的 p95/p99 约为 Go 的 **~4.3×–4.4×**
- Redis(6379) 的 max 达到 30ms+，尾部更长；Go(6666) max ~12ms，更集中。

---

## 4. 资源占用快照（Windows 原生进程）

通过端口定位 PID 后得到的监控表（同一时间点快照）：

| 进程 | PID | CPU(%) | 内存(MB) | 线程数 |
|---|---:|---:|---:|---:|
| Go 服务 | 171260 | 17.734375 | 355.98 | 285 |
| redis-server | 160596 | 26.515625 | 512.98 | 3 |

**观察**
- 在该快照下，Redis 的 CPU% 与内存都更高，但吞吐更低（与 benchmark 结论方向一致：单位请求成本更高/或受环境因素限制更强）。
- Go 服务线程数显著偏高（285）。这通常意味着存在较多阻塞点或 runtime 为避免阻塞而增加 OS 线程。

---

## 5. 关键现象与分析

### 5.1 高并发（c=10000）下延迟飙升的本质
- 该阶段测到的 50ms / 300ms 级别延迟，主要是**请求排队等待**造成。
- 因为 Go(6666) 的服务速率更高，所以队列更短、延迟更低；Redis(6379) 速率更低、队列更长、延迟更高。

### 5.2 随机 Key + 更大 payload 后差距仍然存在
- 在 61B payload、1,000,000 请求规模下，Go(6666) 仍保持 ~94k rps，Redis(6379) ~36.7k rps。
- 说明差距不是“3B 极端小包”造成的单一假象；在更贴近实际负载的 SET 写入场景下仍成立。

---

## 6. 总结

- 在当前实验口径下，Go(6666) 在 **吞吐与延迟（尤其 p95/p99）** 上均显著优于 Redis(6379)：
  - 低并发（c=25）吞吐 **~2.2×–2.6×**；
  - 极高并发连接（c=10000）吞吐 **~3.5×**，Redis 排队延迟显著更重；
  - 随机 Key + 61B payload（c=50, n=1e6）吞吐 **~2.57×**，尾延迟优势达到 **~4×** 级别。
- Windows 资源快照也显示 Redis 在该时刻 CPU/内存更高。
- Go 服务线程数偏高值得进一步排查（可能影响在更复杂功能或更高负载下的稳定性）。

---

## 附录：本次使用的关键命令

### 基础对比（c=25, n=100000）
```bash
redis-benchmark -h 172.31.0.1 -p 6666 -n 100000 -c 25 -t set
redis-benchmark -h 172.31.0.1 -p 6666 -n 100000 -c 25 -t get
redis-benchmark -h 172.31.0.1 -p 6666 -n 100000 -c 25 -t hset

redis-benchmark -h 172.31.0.1 -p 6379 -n 100000 -c 25 -t set
redis-benchmark -h 172.31.0.1 -p 6379 -n 100000 -c 25 -t get
redis-benchmark -h 172.31.0.1 -p 6379 -n 100000 -c 25 -t hset
